{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arlington 2050 Wrap Up\n",
    "\n",
    "## Summary\n",
    "\n",
    "- I participated in the Arlington 2050 project, which is a project created by Arlington County in Virginia, to obtain information from residents about Arlington's future. The county set up different polling stands where residents were given a post card and asked to answer a couple questions. \n",
    "- The questions were, \"Share your message from the future here!\", \"Getting here wasn't easy, but it was worth it! Here is how we did it:\"\n",
    "- The part I played in this whole project was analyising all the data, and creating visuals with it.\n",
    "- I specifically worked on the data collected from the county fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin writing the code. First lets import pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load the excel file with all our data into pandas, and lets view the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = pd.read_excel(\"../../CountyFair.xlsx\")\n",
    "array1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some unnamed columns with data, so lets change their names to accurately match the data they store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = array1.rename(columns={ \"Unnamed: 1\": \"Year2050\", \"Unnamed: 2\": \"Translation1\", \"Unnamed: 3\": \"Getting_Here\",\"Unnamed: 4\": \"Translation2\"})\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the columns all have names, we can begin our analysis. Lets import some libraries that can help us analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking through the data, some of the resposes were in spanish. To avoid errors, lets replace them with their translated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "string_list = ds['Year2050'].tolist()\n",
    "spanish_list = ds['Translation1'].tolist()\n",
    "IndexCounter = 0\n",
    "for n in spanish_list: # Gets non null values from the spanish translated list, if they're not null then it appends to the corresponding index of the main list.\n",
    "    workingstring = str(n)\n",
    "    if workingstring != 'nan':\n",
    "        string_list[IndexCounter] = workingstring\n",
    "    IndexCounter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can seperate all of the words used in our data, this will allow us to find the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ds['Year2050'].str.cat(sep='')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.text for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our most common word list, we can create a wordcloud to visualize what words were the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, mask=None, contour_width=3, contour_color='steelblue').generate(\" \".join(words))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations we can make from these word clouds are the main topics or issues that the residents brought up. These are \"housing\", \"affordability\", \"parks\", \"school\", and \"community\". These five words are the core values and issues of a majority of Arlington residents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the data proided is by using histograms.\n",
    "\n",
    "To start lets use visualize the polarities and subjectivities of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_list = []\n",
    "sub_list = []\n",
    "t = 0\n",
    "for t in range(2,len(string_list)):\n",
    "    text = string_list[t]\n",
    "    doc = nlp(text)\n",
    "    pol_list.append(doc._.blob.polarity)\n",
    "    sub_list.append(doc._.blob.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, I created a list of the polarities and subjectivities of each individual response and stored them in a list. Now that we have the data, we can import the libraries needed to create the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a histogram out of our data. Lets start with the subjectivity of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=sub_list)\n",
    "plt.title('Subjectivity of Postcard Responses from County Fair')\n",
    "plt.xlabel('In a range from 0 to 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the polarities of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=pol_list)\n",
    "plt.title('Polarity of Postcard Responses from County Fair')\n",
    "plt.xlabel('In a range from -1 to 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demensionality Reduction\n",
    "\n",
    "- Dimensionality reduction is a method for representing a given dataset using a lower number of features (i.e. dimensions) while still capturing the original data's meaningful properties.\n",
    "- The reason we do this is to remove irrelevant or redundant features, or simply noisy data, to create a model with a lower number of variables.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In summary, this project was used to gather and analyze responses from citizens living in Arlington Virginia. Through this project, I learned the different ways we can visualize data, and how to use pandas to clean, and adapt data to my needs. This project was fun as it was a real world use of the coding we are learning in school. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
