{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Dimensionality Reduction?\n",
    "- Dimensionality reduction is a method for representing a given dataset using a lower number of features (i.e. dimensions) while still capturing the original data's meaningful properties.\n",
    "- The reason we do this is to remove irrelevant or redundant features, or simply noisy data, to create a model with a lower number of variables.\n",
    "\n",
    "### Word Bank\n",
    "- ***Features***: They are often called variables and refer to the individual measureable properties or characteristics of the data. For example in a dataset about cars, features could be attributes like engine size, horesepower, weight, fuel efficiencey, anything measureable.\n",
    "\n",
    "- ***Noisy data***: Irrelevant, random, or erroneous information that obscures the underlying patters or relationships in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we use Dimensionality Reduction? \n",
    "It is used for several reasons in data analysis and machine learning, but it is primarily used to simplify data.\n",
    "### Key Reasons\n",
    "1. ***Reducing Overfitting***: In high-dimensional datasets, models try to overfit data because they want to try to fit the noisy data, which causes the underlying patterns of the dataset to be lost.\n",
    "\n",
    "2. ***Improving Model Performance***: Reducing the number of features can lead to faster and more efficient machine learning algorithms.\n",
    "\n",
    "3. ***Easier Data Visualization***: It is hard to visualize data with more than 3 features(dimensions), and reducing the demensions makes it easier to explore visual patterns.\n",
    "\n",
    "4. ***Noise Reduction***: Removes a lot of noisy data, to focus on only the important data points.\n",
    "\n",
    "5. ***The Curse of Dimensionality***: As the amount of features grows, the amount of data to ensure the data set is reliable grows exponentialy. This requires more space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Dimensionality Reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "- The most common dimensionality reduction method.\n",
    "- It combines and transforms the data set's feature to produce new features.\n",
    "- These are called principal components.\n",
    "- The principal components together comprise the majority or all the variance present in the original data set.\n",
    "- PCA then projects data onto a new space defined by these new features.\n",
    "- Focuses on data variance\n",
    "###  \n",
    "***Example***:\n",
    "- We have a dataset about snakes with four variables:   \n",
    "    - body length (X1), \n",
    "    - body diameter at widest point (X2) \n",
    "    - fang length (X3), \n",
    "    - weight (X4), \n",
    "    - age (X5). \n",
    "###  \n",
    "Of course, some of these five features may be correlated, such as body length, diameter, and weight.\n",
    "\n",
    "By reducing these data points we can create a data set with less vairables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% pip3 install -U scikit-learn # Get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets  # to retrieve the iris Dataset\n",
    "import pandas as pd  # to load the dataframe\n",
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "from sklearn.decomposition import PCA  # to apply PCA\n",
    "import seaborn as sns  # to plot the heat maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA is similar to PCA as the new features are derived from the initial model. \n",
    "### \n",
    "- However, LDA is different as it focuses not only on data variance but class difference as well.\n",
    "## \n",
    "- One goal of LDA is to maximize interclass difference while minimizing intraclass difference.\n",
    "#\n",
    "- Otherwise LDA is almost the same as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "- https://www.ibm.com/topics/dimensionality-reduction#:~:text=Dimensionality%20reduction%20is%20a%20method,a%20lower%20number%20of%20variables\n",
    "## \n",
    "- https://en.wikipedia.org/wiki/Dimensionality_reduction \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
